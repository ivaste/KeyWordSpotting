{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Prepare X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LoadAndPreprocessDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 120, 126) (1000, 120, 126)\n",
      "(1000, 120, 126, 1) (1000, 120, 126, 1)\n",
      "X_train has 1000 on 30769 files\n",
      "X_val has 1000 on 3703 files\n"
     ]
    }
   ],
   "source": [
    "#Load filenames\n",
    "train,val,test = LoadAndPreprocessDataset.loadDatasetFilenames()\n",
    "\n",
    "#Load TRAIN files\n",
    "X,y_train=LoadAndPreprocessDataset.loadBatch(train,batch_size=1000)\n",
    "#Preprocess TRAIN\n",
    "X_train=LoadAndPreprocessDataset.MFCC_DELTA(X,n_mfcc=40)\n",
    "#X_train=LoadAndPreprocessDataset.MFCC(X,n_mfcc=40)\n",
    "#X_train=LoadAndPreprocessDataset.melspect(X)\n",
    "#Release memory\n",
    "del(X)\n",
    "\n",
    "#Load VAL files\n",
    "X,y_val=LoadAndPreprocessDataset.loadBatch(val,batch_size=1000)\n",
    "#Preprocess VAL\n",
    "X_val=LoadAndPreprocessDataset.MFCC_DELTA(X,n_mfcc=40)\n",
    "#X_val=LoadAndPreprocessDataset.MFCC(X,n_mfcc=40)\n",
    "#X_val=LoadAndPreprocessDataset.melspect(X)\n",
    "#Release memory\n",
    "del(X)\n",
    "\n",
    "print(X_train.shape,X_val.shape)\n",
    "\n",
    "\n",
    "# NORMALIZATION\n",
    "### NO normalization seems better\n",
    "#from sklearn.preprocessing import normalize\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler(feature_range=(-1,1)) # Gives huge bias\n",
    "#for i in range(len(X_train)):\n",
    "    #X_train[i]=normalize(X_train[i], axis=0, norm='max') #normalize each feature of the example\n",
    "    #X_train[i]=(X_train[i] - X_train[i].min(0)) / X_train[i].ptp(0) # HUGE BIAS\n",
    "    #X_train[i]=scaler.fit_transform(X_train[i]) # HUGE BIAS\n",
    "#for i in range(len(X_val)):\n",
    "    #X_val[i]=normalize(X_val[i], axis=0, norm='max') #normalize each feature of the example\n",
    "    #X_val[i]=(X_val[i] - X_val[i].min(0)) / X_val[i].ptp(0) #HUGE BIAS\n",
    "    #X_val[i]=scaler.fit_transform(X_val[i])  # HUGE BIAS\n",
    "\n",
    "\n",
    "#ADD extra dimension for CNN\n",
    "import numpy as np\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "print(X_train.shape,X_val.shape)\n",
    "\n",
    "\n",
    "print('X_train uses',X_train.shape[0],\"of\",len(train),\"files\")\n",
    "print('X_val uses',X_val.shape[0],\"of\",len(val),\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some data\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(X_train[0,:,:,0])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(X_val[0,:,:,0])\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "print(np.min(X_train))\n",
    "print(np.max(X_train))\n",
    "\n",
    "'''for i in range(len(X_train[1])):\n",
    "    print(i,np.min(X_train[1,i]),np.max(X_train[1,i]))\n",
    "print(X_train[1].shape)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "# 2) Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['yes','no','up','down','left','right','on','off','stop','go']\n",
    "nCategories=len(categories)\n",
    "\n",
    "\n",
    "import datetime\n",
    "modelName=\"AttRNNSpeechModel\"\n",
    "save_name=datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")+\"_\"+modelName\n",
    "\n",
    "model = Models.LeNet5(nCategories,\n",
    "                        inputShape=X_train.shape[1:],\n",
    "                        name=modelName)\n",
    "model = Models.AttRNNSpeechModel(nCategories,\n",
    "                        inputShape=X_train.shape[1:],\n",
    "                        name=modelName)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=['sparse_categorical_crossentropy'],\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "import math\n",
    "\n",
    "\n",
    "log_dir = \"logs\\\\\"+save_name\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.4\n",
    "    epochs_drop = 15.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    if (lrate < 4e-5):\n",
    "        lrate = 4e-5\n",
    "      \n",
    "    print('Changing learning rate to {}'.format(lrate))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "def performance_scheduling(epoch):\n",
    "    #.....\n",
    "    return lrate\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_sparse_categorical_accuracy',\n",
    "                  patience=20,\n",
    "                  verbose=1,\n",
    "                  restore_best_weights=True),\n",
    "    ModelCheckpoint('models/'+save_name+'.h5',\n",
    "                    monitor='val_sparse_categorical_accuracy',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True),\n",
    "    TensorBoard(log_dir=log_dir,histogram_freq=1)]\n",
    "    #lrate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "results = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    batch_size=32, # usually between 10 and 32\n",
    "                    epochs=60,\n",
    "                    callbacks=my_callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "#model.save('models/'+modelName+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_dir=\"output/\"+save_name+\"_\"\n",
    "\n",
    "# summarize history for categorical accuracy\n",
    "plt.plot(results.history['sparse_categorical_accuracy'])\n",
    "plt.plot(results.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Categorical accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(save_dir+\"Categorical Accuracy\", dpi=400)\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(save_dir+\"Loss\", dpi=400)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load TEST files\n",
    "X,y_test=LoadAndPreprocessDataset.loadBatch(val,batch_size=3000)\n",
    "#Preprocess TEST\n",
    "X_test=LoadAndPreprocessDataset.MFCC_DELTA(X,n_mfcc=40)\n",
    "#X_test=LoadAndPreprocessDataset.MFCC(X,n_mfcc=40)\n",
    "#X_test=LoadAndPreprocessDataset.melspect(X)\n",
    "#Release memory\n",
    "del(X)\n",
    "\n",
    "#ADD extra dimension for CNN\n",
    "import numpy as np\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('models/'+save_name+'.h5')\n",
    "\n",
    "# Test the model on test data\n",
    "import numpy as np\n",
    "score = model.evaluate(X_test, y_test)\n",
    "y_pred=np.argmax(model.predict(X_test),1)\n",
    "\n",
    "\n",
    "#Measure time for every single prediction\n",
    "\"\"\"import time\n",
    "\n",
    "s=time.time()\n",
    "y_pred=np.argmax(model.predict(X_test[:500,:,:]),1)\n",
    "e=time.time()\n",
    "print(\"Entire:\",e-s)\n",
    "\n",
    "starts=np.empty((len(X_test),))\n",
    "ends=np.empty((len(X_test),))\n",
    "for i in range(len(X_test)):\n",
    "    a=np.array([X_test[i]])\n",
    "    starts[i]=time.time()\n",
    "    a=np.argmax(model.predict(a),1)\n",
    "    ends[i]=time.time()\n",
    "average=sum(ends-starts)/len(X_test)\n",
    "print(\"Average single prediction time (s):\",average)\"\"\"\n",
    "\n",
    "# 1 prediction or a batch of 10 requires the same amount of time\n",
    "# a batch of 100 prediction requires the double of 1\n",
    "# a batch of 500 prediction requires the 3 times of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=results.history.get('loss')[-1]\n",
    "val_loss=results.history.get('val_loss')[-1]\n",
    "\n",
    "train_error=1-results.history.get('sparse_categorical_accuracy')[-1]\n",
    "val_error=1-results.history.get('val_sparse_categorical_accuracy')[-1]\n",
    "\n",
    "test_loss=score[0]\n",
    "test_error=1-score[1]\n",
    "\n",
    "print('Train loss:\\t', round(train_loss,3))\n",
    "print('Train accuracy:\\t', round(1-train_error,3))\n",
    "print('Val loss:\\t', round(val_loss,3))\n",
    "print('Val accuracy:\\t', round(1-val_error,3))\n",
    "print('Test loss:\\t', round(test_loss,3))\n",
    "print('Test accuracy:\\t', round(score[1],3))\n",
    "\n",
    "optimal_error=0.00 #human error\n",
    "\n",
    "bias=train_error-optimal_error\n",
    "variance=val_error-train_error\n",
    "print(\"Bias:\\t\\t\",round(bias,3))\n",
    "print(\"Variance:\\t\",round(variance,3))\n",
    "\n",
    "\n",
    "# Precision and Recall(sensitivity/true positive rate)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "prec=precision_score(y_test, y_pred,average='micro')\n",
    "rec=recall_score(y_test, y_pred,average='micro')\n",
    "\n",
    "#F1 - high if both recall and precision are high.\n",
    "from sklearn.metrics import f1_score\n",
    "f1=f1_score(y_test, y_pred,average='micro')\n",
    "\n",
    "print(\"Precision:\\t\",round(prec,3))\n",
    "print(\"Recall:\\t\\t\",round(rec,3))\n",
    "print(\"F1:\\t\\t\",round(f1,3))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "import audioUtils\n",
    "audioUtils.plot_confusion_matrix(cm,categories, normalize=False,save_path=save_dir)\n",
    "\n",
    "\n",
    "\n",
    "# SAVE MODEL SUMMARY and METRICS TO FILE\n",
    "f = open(save_dir+\"Metrics.txt\", \"a\")\n",
    "\n",
    "stringlist = []\n",
    "model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "short_model_summary = \"\\n\".join(stringlist)\n",
    "f.write(short_model_summary+\"\\n\\n\")\n",
    "\n",
    "f.write('Train loss:\\t'+ str(round(train_loss,3))+ \"\\n\")\n",
    "f.write('Train accuracy:\\t'+ str((round(1-train_error,3)))+ \"\\n\")\n",
    "f.write('Val loss:\\t'+ str(round(val_loss,3))+ \"\\n\")\n",
    "f.write('Val accuracy:\\t'+ str((round(1-val_error,3)))+ \"\\n\")\n",
    "f.write('Test loss:\\t'+ str(round(test_loss,3))+ \"\\n\")\n",
    "f.write('Test accuracy:\\t'+ str(round(score[1],3))+ \"\\n\")\n",
    "f.write(\"Bias:\\t\\t\"+str(round(bias,3))+ \"\\n\")\n",
    "f.write(\"Variance:\\t\"+str(round(variance,3))+ \"\\n\")\n",
    "f.write(\"Precision:\\t\"+str(round(prec,3))+ \"\\n\")\n",
    "f.write(\"Recall:\\t\\t\"+str(round(rec,3))+ \"\\n\")\n",
    "f.write(\"F1:\\t\\t\"+str(round(f1,3)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
